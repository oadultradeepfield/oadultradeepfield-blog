<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="http://localhost:1313//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="http://localhost:1313//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313//apple-touch-icon.png">

<meta name="description" content=""/>



<title>
    
     | Phanuphat Srisukhawasu
    
</title>

<link rel="canonical" href="http://localhost:1313/2024-11-25-st1131-cheat-sheet/"/>

<meta property="og:url" content="http://localhost:1313/2024-11-25-st1131-cheat-sheet/">
  <meta property="og:site_name" content="Phanuphat Srisukhawasu">
  <meta property="og:title" content="Phanuphat Srisukhawasu">
  <meta property="og:description" content="ST1131 Introduction to Statistics and Statistical Computing # Concepts I Often Forget or Get Mixed Up
Note: Taken in AY2024/25 Semester 1
Central Tendency and Variability # Mean, standard deviation, and variance can be sensitive to outliers, while the median and interquartile range are more robust and less affected by them. When you transform data linearly from $X$ to $Y = aX &#43; b$, the mean changes from $\overline{X}$ to $a\overline{X} &#43; b$, and the variance changes from $S^2$ to $a^2 S^2$. For two random variables $X$ and $Y$, there are few interesting properties to note: Linearity of Expectation: $E(X\pm Y)=E(X)\pm E(Y)$. Linearity of Variance: $\text{Var}(X\pm Y)=\text{Var}(X)&#43;\text{Var}(Y)$. Conditional Probability # Conditional probability is the probability of an event $A$ occurring given that another event $B$ has already occurred. It’s expressed as:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:tag" content="Cheatsheet">













<link rel="stylesheet" href="/assets/combined.min.118006e8f5c589e5b687b575b3669e6f8a52b74fa6e3e920dce1c8ef773d6a12.css" media="all">





</head>







<body class="auto">

  <div class="content">
    <header>
      

<div class="header">

    

    <h1 class="header-title">
        <a href="http://localhost:1313/">Phanuphat Srisukhawasu</a>
    </h1>

    <div class="flex">
        

        
        
      
        <p class="small ">
            <a href="/" >
                /home
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/posts" >
                /posts
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/about" >
                /about
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/slides" >
                /slides
            </a>
        </p>
        
        
    </div>

    

</div>

    </header>

    <main class="main">
      





<div class="breadcrumbs">
    
    <a href="/">Home</a>
    <span class="breadcrumbs-separator"> / </span>
    
    <a href="/2024-11-25-st1131-cheat-sheet/"></a>
</div>



<div >

  <div class="single-intro-container">

    

    <h1 class="single-title"></h1>
    

    

    <p class="single-readtime">
      

      
    </p>

  </div>

  

  

  

  

  <div class="single-content">
    <h1 class="heading" id="st1131-introduction-to-statistics-and-statistical-computing">
  ST1131 Introduction to Statistics and Statistical Computing
  <a class="anchor" href="#st1131-introduction-to-statistics-and-statistical-computing">#</a>
</h1>
<blockquote>
<p><em>Concepts I Often Forget or Get Mixed Up</em></p></blockquote>
<p><strong>Note</strong>: Taken in AY2024/25 Semester 1</p>
<h2 class="heading" id="central-tendency-and-variability">
  Central Tendency and Variability
  <a class="anchor" href="#central-tendency-and-variability">#</a>
</h2>
<ul>
<li><strong>Mean</strong>, <strong>standard deviation</strong>, and <strong>variance</strong> can be sensitive to outliers, while the <strong>median</strong> and <strong>interquartile range</strong> are more robust and less affected by them.</li>
<li>When you transform data linearly from $X$ to $Y = aX + b$, the <strong>mean</strong> changes from $\overline{X}$ to $a\overline{X} + b$, and the <strong>variance</strong> changes from $S^2$ to $a^2 S^2$.</li>
<li>For two <strong>random variables</strong> $X$ and $Y$, there are few interesting properties to note:
<ul>
<li><strong>Linearity of Expectation</strong>: $E(X\pm Y)=E(X)\pm E(Y)$.</li>
<li><strong>Linearity of Variance</strong>: $\text{Var}(X\pm Y)=\text{Var}(X)+\text{Var}(Y)$.</li>
</ul>
</li>
</ul>
<hr>
<h2 class="heading" id="conditional-probability">
  Conditional Probability
  <a class="anchor" href="#conditional-probability">#</a>
</h2>
<p>Conditional probability is the probability of an event $A$ occurring given that another event $B$ has already occurred. It’s expressed as:</p>
<p>$$
P(A | B) = \frac{P(A \cap B)}{P(B)}
$$</p>
<h3 class="heading" id="law-of-total-probability">
  Law of Total Probability
  <a class="anchor" href="#law-of-total-probability">#</a>
</h3>
<p>The <strong>Law of Total Probability</strong> helps us find the probability of an event $A$ by considering all possible ways it could occur, broken down by other events ($B_1, B_2, &hellip;, B_n$):</p>
<p>$$
P(A) = P(A \cap B_1) + P(A \cap B_2) + &hellip; + P(A \cap B_n)
$$</p>
<h3 class="heading" id="bayes-theorem">
  Bayes’ Theorem
  <a class="anchor" href="#bayes-theorem">#</a>
</h3>
<p><strong>Bayes&rsquo; Theorem</strong> is a formula that helps us update the probability of an event $B_i$ based on new evidence ($A$). It’s written as:</p>
<p>$$
P(B_i | A) = \frac{P(A | B_i) P(B_i)}{P(A | B_1) P(B_1) + P(A | B_2) P(B_2) + &hellip; + P(A | B_n) P(B_n)}
$$</p>
<h3 class="heading" id="some-notes-on-probability-concepts">
  Some Notes on Probability Concepts
  <a class="anchor" href="#some-notes-on-probability-concepts">#</a>
</h3>
<ul>
<li>Events $A$ and $B$ are said to be <strong>independent</strong> if and only if $P(A\cap B)=P(A)P(B)$.</li>
<li>Events $A$ and $B$ are said to be <strong>(mutually) disjoint</strong> events if and only if $P(A\cap B)=0$, which means the events <strong>cannot happen together</strong>.</li>
</ul>
<hr>
<h2 class="heading" id="binomial-distribution">
  Binomial Distribution
  <a class="anchor" href="#binomial-distribution">#</a>
</h2>
<p>The number of ways to choose $k$ successes out of $n$ trials is given by the combination formula:</p>
<p>$$
\binom{n}{k} = \frac{n!}{k!(n-k)!}
$$</p>
<h3 class="heading" id="3-conditions-for-binomial-distribution">
  3 Conditions for Binomial Distribution:
  <a class="anchor" href="#3-conditions-for-binomial-distribution">#</a>
</h3>
<ol>
<li>There are $n$ trials, each with two possible outcomes (success or failure).</li>
<li>Each trial has a probability $p$ of success.</li>
<li>All trials are independent of each other.</li>
</ol>
<h3 class="heading" id="binomial-random-variable">
  Binomial Random Variable
  <a class="anchor" href="#binomial-random-variable">#</a>
</h3>
<ul>
<li>
<p>The number of successes in $n$ trials is modeled by a <strong>Binomial Distribution</strong>: $Bin(n, p)$.</p>
</li>
<li>
<p>A <strong>Bernoulli Distribution</strong> is a special case of the binomial distribution when $n = 1$: $Bin(1, p)$. The sum of independent Bernoulli random variables follows a <strong>Binomial Distribution</strong>.</p>
</li>
</ul>
<h3 class="heading" id="binomial-formula-for-x-sim-textbinn-p">
  Binomial Formula (for $X \sim \text{Bin}(n, p)$):
  <a class="anchor" href="#binomial-formula-for-x-sim-textbinn-p">#</a>
</h3>
<ul>
<li>The probability of exactly $x$ successes is given by: $$P(X = x) = \binom{n}{x} p^x (1 - p)^{n - x}$$</li>
<li>The <strong>Expected Value</strong> (mean) of $X$ is:
$E(X) = np$.</li>
<li>The <strong>Variance</strong> of $X$ is: $\text{Var}(X) = np(1 - p)$.</li>
</ul>
<hr>
<h2 class="heading" id="point-estimate">
  Point Estimate
  <a class="anchor" href="#point-estimate">#</a>
</h2>
<ul>
<li>$\overline{X} \to \mu$ and $\hat{p} \to p$ represent point estimates.</li>
<li>Point estimates don&rsquo;t show how close they are to the true value (<strong>population parameters</strong>).</li>
</ul>
<hr>
<h2 class="heading" id="standard-error-and-margin-of-error">
  Standard Error and Margin of Error
  <a class="anchor" href="#standard-error-and-margin-of-error">#</a>
</h2>
<p>These terms are quite similar, but they differ slightly in how they’re used:</p>
<ul>
<li><strong>Standard Error (SE)</strong> is the standard deviation of the measured sample mean. A lower <strong>SE</strong> indicates <strong>more accurate results</strong>.</li>
<li><strong>Margin of Error (MOE)</strong> represents the range within which we expect the true population parameter to fall, given a certain confidence level. A smaller <strong>MOE</strong> suggests more <strong>precise estimates</strong>.</li>
</ul>
<hr>
<h2 class="heading" id="confidence-intervals-ci">
  Confidence Intervals (CI)
  <a class="anchor" href="#confidence-intervals-ci">#</a>
</h2>
<p>In the long run, <strong>95% of intervals</strong> will contain the true population parameter. <strong>CI = Point Estimate $\pm$ Margin of Error.</strong> The width of confidence interval ($D$) $=2\times$ MOE.</p>
<h3 class="heading" id="confidence-interval-for-proportion">
  Confidence Interval for Proportion
  <a class="anchor" href="#confidence-interval-for-proportion">#</a>
</h3>
<p>To find the CI given a confidence level ($x$):</p>
<ol>
<li>Calculate $\hat{p}$ and check if $n\hat{p}(1 - \hat{p}) \geq 5$.</li>
<li>Let $\alpha = 1 - x$.</li>
<li>CI $=\hat{p} \pm Z_{1-\frac{\alpha}{2}} \times \sqrt{\displaystyle\frac{\hat{p}(1 - \hat{p})}{n}}$.</li>
</ol>
<p><strong>Determine Sample Size ($n$) Before Study</strong>:</p>
<ol>
<li>Decide on the confidence level ($x$) and the width of the CI ($D$).</li>
<li>Use the formula:
$n \geq \left(\displaystyle\frac{2Z_{1-\frac{\alpha}{2}}}{D}\right)^2 \times \hat{p}(1 - \hat{p})$, where $\hat{p} = 0.5$.</li>
</ol>
<h3 class="heading" id="confidence-interval-for-mean">
  Confidence Interval for Mean
  <a class="anchor" href="#confidence-interval-for-mean">#</a>
</h3>
<p>t-distribution ($t_{\text{df}}$) approaches $N(0, 1)$ as degrees of freedom ($df$) increase.
To find the CI given a confidence level ($x$):</p>
<ol>
<li><strong>Assumptions</strong>: The sample is random (not biased), and the data distribution is symmetric (or the sample size is large enough).</li>
<li>CI $=\overline{X} \pm t_{n-1; 1-\frac{\alpha}{2}} \times \displaystyle\frac{s}{\sqrt{n}}$.</li>
</ol>
<p><strong>Determine Sample Size ($n$) Before Study</strong>:</p>
<ol>
<li>Decide on the confidence level ($x$) and the width of the CI ($D$).</li>
<li>Use the formula:
$n \geq \left(\displaystyle\frac{2Z_{1-\frac{\alpha}{2}} \times s}{D}\right)^2$.</li>
<li>For $s$, look for similar studies (as given in the context). Ensure $n \geq 30$ to apply the t-distribution comfortably.</li>
</ol>
<hr>
<h2 class="heading" id="hypothesis-testing">
  Hypothesis Testing
  <a class="anchor" href="#hypothesis-testing">#</a>
</h2>
<p>Hypothesis testing is all about making decisions based on data. The main idea is to test a <strong>null hypothesis</strong> ($H_0$) against an <strong>alternative hypothesis</strong> ($H_1$). Here&rsquo;s a breakdown of the key terms:</p>
<ul>
<li><strong>Null hypothesis ($H_0$)</strong> vs. <strong>Alternative hypothesis ($H_1$)</strong></li>
<li><strong>Test statistic</strong>: How far the point estimate is from our initial guess (the null hypothesis).</li>
<li><strong>Null distribution</strong>: The distribution of the test statistic under $H_0$.</li>
<li><strong>p-Value</strong>: This tells you how unlikely your observed result is if $H_0$ is true.</li>
<li><strong>Significance level ($\alpha$)</strong>: If the p-Value is less than or equal to α, you reject $H_0$.</li>
<li>A test is statistically significant when we reject $H_0$.</li>
<li><strong>Type I Error</strong>: Rejecting $H_0$ when it&rsquo;s actually true.</li>
<li><strong>Type II Error</strong>: Not rejecting $H_0$ when it is false.</li>
<li>Increasing your sample size can help reduce both types of errors.</li>
</ul>
<h3 class="heading" id="one-sample-proportion">
  One Sample, Proportion
  <a class="anchor" href="#one-sample-proportion">#</a>
</h3>
<p>Here’s how you would test a proportion in one sample:</p>
<ol>
<li><strong>Assumptions</strong>: The data is categorical, random, and we have $np_0(1 - p_0) \geq 5$.</li>
<li><strong>Hypothesis</strong>:
<ul>
<li>$H_0: p = p_0$</li>
<li>$H_1: p \neq p_0$</li>
</ul>
</li>
<li><strong>Test statistic</strong>:
$$
z = \displaystyle\frac{p - p_0}{\sqrt{\frac{p_0(1 - p_0)}{n}}}
$$
where $z \sim N(0,1)$.</li>
<li><strong>p-Value</strong>:
<ul>
<li>For a right-sided test: $P(z \geq \text{Test stat.} \| z \sim N(0,1))$</li>
<li>For a two-sided test: $2 \times P(z \geq \text{Test stat.} \| z \sim N(0,1))$</li>
</ul>
</li>
<li><strong>Conclusion</strong>:<br>
If the p-Value $\leq \alpha$, reject $H_0$. Otherwise, you cannot reject $H_0$.</li>
</ol>
<h3 class="heading" id="one-sample-mean">
  One Sample, Mean
  <a class="anchor" href="#one-sample-mean">#</a>
</h3>
<p>Testing the mean of one sample:</p>
<ol>
<li><strong>Assumptions</strong>: The data is quantitative, random, and normally distributed (or $n \geq 30$).</li>
<li><strong>Hypothesis</strong>:
<ul>
<li>$H_0: \mu = \mu_0$</li>
<li>$H_1: \mu \neq \mu_0$</li>
</ul>
</li>
<li><strong>Test statistic</strong>:
$$
T = \displaystyle\frac{\overline{X} - \mu_0}{s / \sqrt{n}}
$$
where $T \sim t_{n-1}(0,1)$.</li>
<li><strong>p-Value and Conclusion</strong>: This works the same as the proportion test.
<ul>
<li>The result of a two-sided test for the mean is equivalent to using a confidence interval.</li>
</ul>
</li>
</ol>
<h3 class="heading" id="two-sample-independent-equal-variance">
  Two Sample, Independent, Equal Variance
  <a class="anchor" href="#two-sample-independent-equal-variance">#</a>
</h3>
<p>When comparing the means of two independent samples with equal variances:</p>
<ol>
<li><strong>Assumptions</strong>: The data is quantitative, random, independent, and the population distribution is approximately normal (or $n$ is large enough). For the equal variance test, $p &gt; 0.05$.</li>
<li><strong>Hypothesis</strong>:
<ul>
<li>$H_0: \mu_1 = \mu_2$</li>
<li>$H_1: \mu_1 \neq \mu_2$</li>
</ul>
</li>
<li><strong>Test statistic</strong>:
$$
T = \displaystyle\frac{\overline{X} - \overline{Y}}{se}
$$
where
$$
se = \displaystyle\sqrt{\frac{s_p^2}{n_1} + \frac{s_p^2}{n_2}}
$$
and
$$
s_p^2 = \displaystyle\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}
$$
(Pooled estimate of common variance).<br>
Where $T \sim t_{n_1 + n_2 - 2}$.</li>
</ol>
<h3 class="heading" id="two-sample-independent-unequal-variance">
  Two Sample, Independent, Unequal Variance
  <a class="anchor" href="#two-sample-independent-unequal-variance">#</a>
</h3>
<p>This is similar to the previous test, but with unequal variances:</p>
<ol>
<li><strong>Assumptions</strong>: Same as above, but the population variances are different.</li>
<li><strong>Hypothesis</strong>:
<ul>
<li>$H_0: \mu_1 = \mu_2$</li>
<li>$H_1: \mu_1 \neq \mu_2$</li>
</ul>
</li>
<li><strong>Test statistic</strong>:
$$
T = \displaystyle\frac{\overline{X} - \overline{Y}}{se}
$$
where
$$
se = \displaystyle\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
$$
and $T \sim t_{\text{df}}$.</li>
</ol>
<h3 class="heading" id="two-sample-dependent">
  Two Sample, Dependent
  <a class="anchor" href="#two-sample-dependent">#</a>
</h3>
<p>When the two samples are dependent (e.g., before and after comparisons):</p>
<ul>
<li>Each observation has a matching pair.</li>
<li>Take the difference of the paired observations and compare the mean of those differences to 0. This is similar to performing a one-sample test.</li>
</ul>
<hr>
<h2 class="heading" id="qq-plot---check-normality">
  QQ Plot - Check Normality
  <a class="anchor" href="#qq-plot---check-normality">#</a>
</h2>
<p>A QQ plot helps check if your data follows a normal distribution.</p>
<ul>
<li><strong>Right tail below/above the line</strong>: This indicates a <strong>longer/shorter</strong> right tail.</li>
<li><strong>Left tail below/above the line</strong>: This indicates a <strong>shorter/longer</strong> left tail.</li>
</ul>

    
  </div>

  

  

  
  

<div class="single-pagination">
    <hr />

    <div class="flex">

        <div class="single-pagination-prev">
            
            <div class="single-pagination-container-prev">
                <div class="single-pagination-text">←</div>
                <div class="single-pagination-text">
                    <a href="/2024-11-28-cs1231s-interesting-problems/">
                        
                    </a>
                </div>
            </div>
            
        </div>

        <div class="single-pagination-next">
            
            <div class="single-pagination-container-next">
                <div class="single-pagination-text">
                    <a href="/2024-11-24-stream-processing/">
                        
                    </a>
                </div>
                <div class="single-pagination-text">→</div>
            </div>
            
        </div>

    </div>

    <hr />
</div>



  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


    </main>
  </div>

  <footer>
    


  </footer>

  

</body>

<script src="/js/theme-switch.js"></script>
<script defer src="/js/copy-code.js"></script>
</html>
