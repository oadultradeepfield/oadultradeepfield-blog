<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="http://localhost:1313//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="http://localhost:1313//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313//apple-touch-icon.png">

<meta name="description" content=""/>



<title>
    
     | Phanuphat Srisukhawasu
    
</title>

<link rel="canonical" href="http://localhost:1313/2025-01-24-useful-stuffs-from-ma1522/"/>

<meta property="og:url" content="http://localhost:1313/2025-01-24-useful-stuffs-from-ma1522/">
  <meta property="og:site_name" content="Phanuphat Srisukhawasu">
  <meta property="og:title" content="Phanuphat Srisukhawasu">
  <meta property="og:description" content="Useful Stuffs from MA1522 # “Life is a linear equation in which you can’t cross multiply! If you think you can do it, you can do it. If you think you can’t do it, you can’t do it.”
— Israelmore Ayivor
Like other series of courses I take in this semester, I note some of the mistakes I made i.e. misconception or some tricky terms here to help me revise faster during the exam period.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:tag" content="NUS-Courses">













<link rel="stylesheet" href="/assets/combined.min.118006e8f5c589e5b687b575b3669e6f8a52b74fa6e3e920dce1c8ef773d6a12.css" media="all">





</head>







<body class="auto">

  <div class="content">
    <header>
      

<div class="header">

    

    <h1 class="header-title">
        <a href="http://localhost:1313/">Phanuphat Srisukhawasu</a>
    </h1>

    <div class="flex">
        

        
        
      
        <p class="small ">
            <a href="/" >
                /home
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/posts" >
                /posts
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/about" >
                /about
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/slides" >
                /slides
            </a>
        </p>
        
        
    </div>

    

</div>

    </header>

    <main class="main">
      





<div class="breadcrumbs">
    
    <a href="/">Home</a>
    <span class="breadcrumbs-separator"> / </span>
    
    <a href="/2025-01-24-useful-stuffs-from-ma1522/"></a>
</div>



<div >

  <div class="single-intro-container">

    

    <h1 class="single-title"></h1>
    

    

    <p class="single-readtime">
      

      
    </p>

  </div>

  

  

  

  

  <div class="single-content">
    <h1 class="heading" id="useful-stuffs-from-ma1522">
  Useful Stuffs from MA1522
  <a class="anchor" href="#useful-stuffs-from-ma1522">#</a>
</h1>
<blockquote>
<p><em>&ldquo;Life is a linear equation in which you can&rsquo;t cross multiply! If you think you can do it, you can do it. If you think you can&rsquo;t do it, you can&rsquo;t do it.&rdquo;</em></p>
<p>— <em>Israelmore Ayivor</em></p></blockquote>
<p>Like other series of courses I take in this semester, I note some of the mistakes I made i.e. misconception or some tricky terms here to help me revise faster during the exam period.</p>
<h2 class="heading" id="equivalent-statements-of-invertibility">
  Equivalent Statements of Invertibility
  <a class="anchor" href="#equivalent-statements-of-invertibility">#</a>
</h2>
<p>These are the most important concepts I&rsquo;ve noticed so far. They link many topics together, and the list continues to grow as new terms are introduced, so I place it at the top of my notes.</p>
<p>For a <strong>square matrix</strong> $\mathbf{A}$ of order $n$, the following statements are equivalent, meaning each one implies the others:</p>
<ol>
<li>$\mathbf{A}$ is invertible.</li>
<li>$\mathbf{A}^T$ is invertible.</li>
<li>$\mathbf{A}$ has a left-inverse.</li>
<li>$\mathbf{A}$ has a right-inverse.</li>
<li>The RREF of $\mathbf{A}$ is the identity matrix.</li>
<li>$\mathbf{A}$ can be expressed as the product of elementary matrices.</li>
<li>$\mathbf{A}\mathbf{x} = \mathbf{0}$ has only the trivial solution.</li>
<li>$\mathbf{A}\mathbf{x} = \mathbf{b}$ has a unique solution (and thus consistent).</li>
<li>$\det(\mathbf{A})\neq 0$.</li>
<li>The columns/rows of $\mathbf{A}$ are linearly independent, form a basis, and, therefore, span $\mathbb{R}^n$.</li>
<li>$\mathbf{A}$ is full rank, that is $\text{rank}(\mathbf{A})=n$.</li>
<li>$\text{nullity}(A)=0$.</li>
<li>0 is not an eigenvalue of $\mathbf{A}$.</li>
<li>The linear transformation $T$ defined by $T_{\mathbf{A}}(\mathbf{v})=\mathbf{Av}$ is both injective and surjective.</li>
</ol>
<hr>
<h2 class="heading" id="chapter-1-linear-systems">
  Chapter 1: Linear Systems
  <a class="anchor" href="#chapter-1-linear-systems">#</a>
</h2>
<ul>
<li>A standard equation is written in the form $a_1x_1 + a_2x_2 + \dots + a_nx_n = b$ (with variables on one side and the constant on the other).</li>
<li>A linear system is <strong>inconsistent</strong> if there is no solution and <strong>consistent</strong> if it has at least one solution.</li>
<li>For 3D linear systems, there are no solutions if at least two of the planes are parallel, or if all three planes intersect but do not form a single line. A unique solution occurs when two of the planes intersect as lines, and those lines intersect at a single point.</li>
<li>Based on the above, the system has infinitely many solutions (a general solution) with one parameter if the three planes intersect along a line, and with two parameters if the planes coincide.</li>
<li>When analyzing augmented matrices, we consider the leading entries, including those on the right-hand side (RHS). So, if the last column (RHS) is a pivot column, there is no solution (generally be $0\ 0\ 0 \ldots\ 0\ \lvert\ b$).</li>
<li>Reduced Row Echelon Form (RREF) differs from Row Echelon Form (REF) in that its leading entries are all 1&rsquo;s. Additionally, a pivot column in RREF contains a 1 in the leading row and 0&rsquo;s elsewhere.</li>
</ul>
<hr>
<h2 class="heading" id="chapter-2-matrix-algebra">
  Chapter 2: Matrix Algebra
  <a class="anchor" href="#chapter-2-matrix-algebra">#</a>
</h2>
<ul>
<li>In this course, a zero matrix can be categorized as a triangular matrix, diagonal matrix, and scalar matrix, as the definitions of these types do not require the diagonal or triangular elements to be nonzero.</li>
<li>A symmetric matrix $\mathbf{A}$ satisfies the property $\mathbf{A} = \mathbf{A}^T$, and $\mathbf{A}^n$ is also symmetric (but not necessarily true for the converse).</li>
<li>Every elementary matrix is invertible, and its inverse corresponds to reversing the row operation performed on $\mathbf{I}_n$ to obtain the elementary matrix.</li>
<li>To get the product $\mathbf{E}\mathbf{A}$, note that the size of $\mathbf{E}$ should be $m \times m$ if $\mathbf{A}$ is $m \times n$.</li>
<li>If $\mathbf{A} \xrightarrow{r_1} \xrightarrow{r_2} \dots \xrightarrow{r_k} \mathbf{B}$, then $\mathbf{B} = \mathbf{E}_k \dots \mathbf{E}_2 \mathbf{E}_1 \mathbf{A}$, which implies $\mathbf{A} = \mathbf{E}_1^{-1} \mathbf{E}_2^{-1} \dots \mathbf{E}_k^{-1} \mathbf{B}$, or equivalently, $\mathbf{B} \xrightarrow{r_k&rsquo;} \dots \xrightarrow{r_2&rsquo;} \xrightarrow{r_1&rsquo;} \mathbf{A}$.</li>
<li>To quickly obtain $\mathbf{L}$ after reducing $\mathbf{A}$ to $\mathbf{U}$, we can perform only the row operation $R_i + cR_j$ for $i &gt; j$. We can then replace the $(i,j)$ entry of $\mathbf{L}$ with the value of $-c$.</li>
<li>$\mathbf{A}\mathbf{x} = \mathbf{b}$ has a unique solution when $\mathbf{U}\mathbf{x} = \mathbf{y}$ has a unique solution, which occurs when $\mathbf{U}$ is invertible, since $\mathbf{L}$ is always invertible.</li>
<li>The determinant of a triangular matrix is equal to the product of the entries along its diagonal.</li>
<li>If a square matrix has two identical rows/columns or if one row/column is a scalar multiple of another, the determinant is zero.</li>
</ul>
<hr>
<h2 class="heading" id="chapter-3-euclidean-vector-spaces">
  Chapter 3: Euclidean Vector Spaces
  <a class="anchor" href="#chapter-3-euclidean-vector-spaces">#</a>
</h2>
<ul>
<li>The dot product between vectors $\mathbf{u}$ and $\mathbf{v}$ is defined as $\mathbf{u}^T\mathbf{v}$ because vectors in this course are column vectors. Consequently, $(\mathbf{u}^T\mathbf{v})^T = \mathbf{v}^T\mathbf{u} = \mathbf{v} \cdot \mathbf{u}$ is also valid, as the dot product is commutative.</li>
<li>$\text{span}(S) = \mathbb{R}^n \iff$ the RREF of the matrix formed by the column vectors in $S$ has no zero rows. To span $\mathbb{R}^n$, we need at least $n$ linearly independent vectors.</li>
<li>To check if $\mathbf{v}$ is a linear combination of the vectors in the set $S$, verify that the equation $\mathbf{A}\mathbf{x} = \mathbf{v}$ has a solution. To determine if the span of one set is a subset of another, check if all vectors in the first set can be expressed as a linear combination of the vectors in the second set.</li>
<li>The solution set of $\mathbf{A}\mathbf{x} = \mathbf{b}$ is given by $\mathbf{u} + V$, where $\mathbf{u}$ is a particular solution of the system, and $V$ is the solution set of the homogeneous system $\mathbf{A}\mathbf{x} = \mathbf{0}$.</li>
<li>All subspaces contain infinitely many vectors due to their recursive definition. However, the zero subspace is the only exception, containing exactly one element: $\mathbf{0}$.</li>
<li>A set of vectors is linearly independent if and only if the homogeneous system, with the vectors as the columns of the coefficient matrix, has only the trivial solution.</li>
<li><strong>Some Special Cases</strong>: The empty set is linearly independent, and a set containing a single vector $\mathbf{v}$ is linearly independent if and only if $\mathbf{v} \neq \mathbf{0}$. The span of all zero vectors is the only case where the span forms a single point (the zero vector).</li>
<li>Linear dependency is defined on the vector set as a whole (when there are more than two vectors) because if one vector can be expressed as a linear combination of the others, arithmetic operations on the vectors can yield similar results.</li>
<li>The dimension of a subspace is determined by the minimum number of vectors needed to form a basis. Note that $\mathbb{R}^2$ is not a subspace of $\mathbb{R}^3$ since it lacks a third coordinate, but we can have a 2D plane that is a two-dimensional subspace of $\mathbb{R}^3$ by setting the third coordinate to $0$.</li>
<li>When expressing coordinates relative to a basis, we always include only as many coordinates as the dimension of the subspace. However, the basis itself should have the same number of entries as the dimension of the superspace.</li>
<li>Both REF and RREF have infinitely many solutions if there are non-pivot columns, as these columns correspond to free variables (parameters), which also determine the dimension of the solution space.</li>
<li>Conditions to check if $S$ is a basis of subspace $V$ (both requires $\lvert S \rvert = \text{dim}(V)$):
<ol>
<li>If $\text{span}(S) = V$ is not known: $S \subseteq V$ and $S$ is linearly independent.</li>
<li>If $S$ being linearly independent is not known: $V \subseteq \text{span}(S)$.</li>
</ol>
</li>
<li>$U \subseteq V \iff \text{dim}(U) \leq \text{dim}(V)$, with equality if and only if $U = V$.</li>
<li>To convert the basis back from the transition matrix, find the inverse of the transition matrix. Note that the transition matrix should be a square matrix with $n$ equal to the dimension.</li>
</ul>
<hr>
<h2 class="heading" id="chapter-4-subspaces-associated-to-a-matrix">
  Chapter 4: Subspaces Associated to a Matrix
  <a class="anchor" href="#chapter-4-subspaces-associated-to-a-matrix">#</a>
</h2>
<ul>
<li>Although an $m \times n$ matrix has $m$ rows and $n$ columns, the row space is a subspace of $\mathbb{R}^n$ (with $n$ coordinates), while the column space is a subspace of $\mathbb{R}^m$ (with $m$ coordinates).</li>
<li>The non-zero rows of the RREF of $\mathbf{A}$ form a basis for $\text{Row}(\mathbf{A})$. However, the corresponding pivot columns of the RREF of $\mathbf{A}$ (denoted as $\mathbf{R}$) form a basis for $\text{Col}(\mathbf{A})$ (it is not preserved under row operations).</li>
<li>The nullspace of $\mathbf{A}$ is the solution space of $\mathbf{A}\mathbf{x} = \mathbf{0}$. The nullity of $\mathbf{A}$ is the dimension of its nullspace, which is essentially the same as number of non-pivot columns its in RREF.</li>
<li>$\text{rank}(\mathbf{A})=\text{dim}(\text{Col}(\mathbf{A}))=\text{dim}(\text{Row}(\mathbf{A}))$ and is preserved under transpose.</li>
<li>$\text{Col}(\mathbf{AB})\subseteq \text{Col}(\mathbf{A})$, $\text{Rank}(\mathbf{AB})\leq \text{min}(\text{rank}(\mathbf{A}), \text{rank}(\mathbf{B}))$, and $\text{rank}(A)+\text{nullity}(\mathbf{A})=n$ for $m\times n$ matrix.</li>
<li>For an $m \times n$ matrix $\mathbf{A}$, the following are equivalent:
<ol>
<li>$\mathbf{A}$ is full rank: $\text{rank}(\mathbf{A}) = n$.</li>
<li>The rows of $\mathbf{A}$ span $\mathbb{R}^n$: $\text{Row}(\mathbf{A}) = \mathbb{R}^n$.</li>
<li>The columns of $\mathbf{A}$ are linearly independent.</li>
<li>The homogeneous system $\mathbf{Ax = 0}$ has only the trivial solution: $\text{Null}(\mathbf{A}) = \text{set}(\mathbf{0})$.</li>
<li>$\mathbf{A}^T \mathbf{A}$ is an invertible matrix of order $n$.</li>
<li>$\mathbf{A}$ has a left inverse.</li>
<li><a href="#chapter-7-linear-transformation">The linear transformation with $\mathbf{A}$ as the standard matrix is injective.</a></li>
</ol>
</li>
<li>For an $m \times n$ matrix $\mathbf{A}$, the following are equivalent:
<ol>
<li>$\mathbf{A}$ is full rank: $\text{rank}(\mathbf{A}) = m$.</li>
<li>The columns of $\mathbf{A}$ span $\mathbb{R}^m$: $\text{Col}(A) = \mathbb{R}^m$.</li>
<li>The rows of $\mathbf{A}$ are linearly independent.</li>
<li>The system $\mathbf{Ax = b}$ is consistent for all $\mathbf{b} \in \mathbb{R}^m$.</li>
<li>$\mathbf{A} \mathbf{A}^T$ is an invertible matrix of order $m$.</li>
<li>$\mathbf{A}$ has a right inverse.</li>
<li><a href="#chapter-7-linear-transformation">The linear transformation with $\mathbf{A}$ as the standard matrix is surjective.</a></li>
</ol>
</li>
</ul>
<hr>
<h2 class="heading" id="chapter-5-orthogonality-and-least-square-solution">
  Chapter 5: Orthogonality and Least Square Solution
  <a class="anchor" href="#chapter-5-orthogonality-and-least-square-solution">#</a>
</h2>
<ul>
<li>A set of vectors is <strong>orthogonal</strong> if they are pairwise orthogonal (i.e., their dot product is zero). They are <strong>orthonormal</strong> if they are orthogonal and all vectors in the set are unit vectors.</li>
<li>If a subspace $V$ is defined by a linear equation, the vector orthogonal to $V$ consists of those that are multiples of the coefficients of the equation.</li>
<li>Any orthogonal set of nonzero vectors is linearly independent (but not vice versa).</li>
<li>A matrix $\mathbf{A}$ is orthogonal if $\mathbf{A}^T = \mathbf{A}^{-1}$. Equivalently, $\mathbf{A}^T \mathbf{A} = \mathbf{I} = \mathbf{A} \mathbf{A}^T$, meaning the columns and rows of $\mathbf{A}$ form an <strong>orthonormal</strong> basis for $\mathbb{R}^n$.</li>
<li>If we can express $\mathbf{v} = c_1 \mathbf{u}_1 + c_2 \mathbf{u}_2 + \dots + c_k \mathbf{u}_k$, where the set of vectors $\mathbf{u}_i$ is an orthogonal basis, remember that each component $c_i$ is $\frac{\mathbf{v} \cdot \mathbf{u}_i}{\Vert\mathbf{u}_i\Vert^2}.$ In this case, we also call $\mathbf{v}$ the orthogonal projection of some vector $\mathbf{w}$, where $\mathbf{w} = \mathbf{v} + \mathbf{w}_n$ and $\mathbf{w}_n$ is orthogonal to the subspace that $\mathbf{v}$ is in. It is also true that $\mathbf{v}$ is the vector in the subspace that is closest to $\mathbf{w}$.</li>
<li><strong>Gram-Schmidt Orthogonalization</strong>: Let $\mathbf{u} _i$ be an element in the linearly independent set. We can create an orthonormal set of $\mathbf{v} _i$ by using $\mathbf{v} _k = \mathbf{u} _k-\sum _{i=1}^{k-1}\frac{\mathbf{v} _i\cdot\mathbf{u} _k}{\Vert\mathbf{v} _i\Vert^2} \mathbf{v} _i$, where $\mathbf{v} _1 = \mathbf{u} _1$. There can be a trick question where $\mathbf{v} _k$ is $\mathbf{0}$ but $\mathbf{v} _{k-1}$ is not. In this case, it indicates that $\mathbf{u} _k$ is not part of a linearly independent set, since we can form $\mathbf{u} _k$ as a linear combination of the previous elements.</li>
<li>If a matrix $\mathbf{A}$ has a linearly independent columns, it can be factorized into $\mathbf{QR}$ where $\mathbf{Q}$ is a <strong>orthonormal</strong> columns and $\mathbf{R}=\mathbf{Q}^T\mathbf{A}$ is an upper triangular matrix with positive diagonal entries. Note that each column in $\mathbf{A}$ is also a multiple of the same column in $\mathbf{Q}$.</li>
<li>A vector $\mathbf{u}$ is a least square solution to $\mathbf{Ax}=\mathbf{b}\iff \mathbf{u}$ is a solution to $\mathbf{A}^T\mathbf{Ax}=\mathbf{A}^T\mathbf{b}$, which is not guaranteed to be unique, but the projection $\mathbf{Au}=\mathbf{A}(\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b}$ (to $\text{Col}(\mathbf{A})$) is unique.</li>
</ul>
<hr>
<h2 class="heading" id="chapter-6-eigenanalysis">
  Chapter 6: Eigenanalysis
  <a class="anchor" href="#chapter-6-eigenanalysis">#</a>
</h2>
<ul>
<li>The characteristic polynomial of $\mathbf{A}$, denoted as $\text{char}(\mathbf{A}) = \det(x\mathbf{I} - \mathbf{A})$, is used to determine the eigenvalues. An eigenvalue $\lambda$ is a root of this polynomial, implying that $(\lambda \mathbf{I} - \mathbf{A})\mathbf{x} = \mathbf{0}$ has nontrivial solutions.</li>
<li>The algebraic multiplicity of $\lambda$ is the largest integer $r_\lambda$ such that $\det(x\mathbf{I} - \mathbf{A}) = (x - \lambda)^{r_\lambda}p(x)$ for some polynomial $p(x)$, where $\lambda$ is not a root of $p(x)$.</li>
<li>If $\text{char}(\mathbf{A})$ can be factored into linear terms, the roots of the polynomial are the eigenvalues.</li>
<li>The eigenvalues of a triangular matrix are its diagonal entries. The algebraic multiplicity of each eigenvalue is the number of times it appears on the diagonal.</li>
<li>The geometric multiplicity $\dim(E_\lambda)$ of an eigenvalue $\lambda$ is the dimension of its associated eigenspace. Each eigenspace is linearly independent, and $1 \leq \dim(E_\lambda) \leq r_\lambda$.</li>
<li>A square matrix $\mathbf{A}$ is diagonalizable, $\mathbf{A} = \mathbf{PDP}^{-1}$, if it has $n$ linearly independent eigenvectors. $\mathbf{D}$ is a diagonal matrix with the eigenvalues as its diagonal entries, and $\mathbf{P}$ contains the corresponding eigenvectors as its columns.</li>
<li>If $\mathbf{A}$ is diagonalizable, then the geometric multiplicity equals the algebraic multiplicity for each eigenvalue.</li>
<li><strong>Spectral Theorem</strong>: $\mathbf{A}$ is orthogonally diagonalizable ($\mathbf{A} = \mathbf{PDP}^T$) <strong>if and only if</strong> $\mathbf{A}$ is symmetric. The eigenspaces of $\mathbf{A}$ are orthogonal.</li>
<li><strong>Power of a Diagonalizable Matrix</strong>: $\mathbf{A}^m = \mathbf{PD}^m\mathbf{P}^{-1}$, where $\mathbf{D}^m$ is obtained by raising each diagonal entry of $\mathbf{D}$ to the power of $m$. This applies to $m = -1$ (inverse) as well.</li>
<li>A steady-state (equilibrium) vector for a stochastic matrix $\mathbf{P}$ is a probability vector that is an eigenvector associated with eigenvalue 1. If a Markov chain converges, it converges to this vector.</li>
<li>To compute the equilibrium vector, find the eigenvector $\mathbf{u}$ associated with eigenvalue 1, then normalize it: $\mathbf{v} = \frac{\mathbf{u}}{\sum_{k=1}^n u_k}$.</li>
<li>The eigenvalues $\mu_i$ of $\mathbf{A}^T\mathbf{A}$ are nonnegative. The singular values of $\mathbf{A}$ are $\sigma_i = \sqrt{\mu_i}$. When ordered in decreasing order, these are the diagonal entries of $\mathbf{\Sigma}$.</li>
<li><strong>Singular Value Decomposition (SVD)</strong>: $\mathbf{A} = \mathbf{U\Sigma V}^T$.
<ol>
<li>Compute the eigenvalues of $\mathbf{A}^T\mathbf{A}$. For eigenvalues with algebraic multiplicity greater than one, repeat them on the diagonal of $\mathbf{\Sigma}$.</li>
<li>$\mathbf{V}$ consists of orthonormal eigenvectors of $\mathbf{A}^T\mathbf{A}$.</li>
<li>Compute $\mathbf{u}_i = \mathbf{A}\mathbf{v}_i / \sigma_i$.</li>
<li>If the set of $\mathbf{u}_i$ does not form an orthonormal basis ($r \neq m$), extend it to complete the basis.</li>
</ol>
</li>
</ul>
<hr>
<h2 class="heading" id="chapter-7-linear-transformation">
  Chapter 7: Linear Transformation
  <a class="anchor" href="#chapter-7-linear-transformation">#</a>
</h2>
<ul>
<li>A mapping $T:\mathbb{R}^n\to\mathbb{R}^m$ is a linear transformation $\iff T(\alpha \mathbf{u} + \beta \mathbf{v}) = \alpha T(\mathbf{u}) + \beta T(\mathbf{v})$. We call $\mathbb{R}^n$ the domain and $\mathbb{R}^m$ the codomain. In general, this can be extended to a set of vectors.</li>
<li>There are many properties of linearity; an easy way to test it is to check whether it maps $\mathbf{0}_n$ to $\mathbf{0}_m$. Specifically, if $T(\mathbf{0}_n) \neq \mathbf{0}_m$, then the transformation is not linear.</li>
<li>A linear transformation exists if we can express $T(\mathbf{u}) = \mathbf{A}\mathbf{u}$. The columns of $\mathbf{A}$ correspond to the images of the standard basis vectors for $\mathbb{R}^n$. The matrix $\mathbf{A}$ is referred to as the standard matrix or the matrix representation of $T$.</li>
<li>If the standard basis is not used, the image can still be constructed using the relationship $T(\mathbf{v}) = [T]_S [\mathbf{v}]_S$. If $\mathbf{P}$ is the transition matrix from the standard basis to $S$, then $\mathbf{A} = [T]_S \mathbf{P}$.</li>
<li>The range of $T$ is the column space of its associated standard matrix $\mathbf{A}$: $\text{Col}(\mathbf{A})$. The rank of $T$ is the dimension of its range, which is equal to $\text{rank}(\mathbf{A})$.</li>
<li>The kernel of $T$ is the set of all vectors whose images under $T$ are $\mathbf{0}$. This is the same as the nullspace of $\mathbf{A}$: $\text{Null}(\mathbf{A})$. The dimension of the kernel is referred to as the nullity.</li>
<li>A linear transformation is injective if $\text{ker}(T) = \text{set}(\mathbf{0})$, meaning that the equation $\mathbf{Ax} = \mathbf{0}$ has only the trivial solution.</li>
<li>A linear transformation is surjective if its range is equal to its codomain. The transformation that maps the larger to smaller spaces is always surjective.</li>
</ul>

    
  </div>

  

  

  
  

<div class="single-pagination">
    <hr />

    <div class="flex">

        <div class="single-pagination-prev">
            
        </div>

        <div class="single-pagination-next">
            
            <div class="single-pagination-container-next">
                <div class="single-pagination-text">
                    <a href="/2025-01-22-cs2100-common-mistakes-part-1/">
                        
                    </a>
                </div>
                <div class="single-pagination-text">→</div>
            </div>
            
        </div>

    </div>

    <hr />
</div>



  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


    </main>
  </div>

  <footer>
    


  </footer>

  

</body>

<script src="/js/theme-switch.js"></script>
<script defer src="/js/copy-code.js"></script>
</html>
